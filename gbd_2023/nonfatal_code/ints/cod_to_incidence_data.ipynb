{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276f926-97f7-4c2e-b479-32b7ee9712c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pymysql \n",
    "import random\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from scipy.special import logit, expit\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "from db_queries import get_age_metadata, get_cod_data, get_covariate_estimates, get_demographics, get_location_metadata\n",
    "from elmo import get_crosswalk_version\n",
    "\n",
    "\n",
    "# Specify run parameters here\n",
    "RELEASE = 16         # The GBD release id for which you're running the code\n",
    "LEVEL_3 = 'ints'     #'ints' # Either 'ints' or 'intest'\n",
    "N_DRAWS = 100        # Number of draws to propogate uncertainty to data point (file does not export draws, so is independant of GBD)\n",
    "YEAR_BIN_WIDTH = 10  # We collapse deaths across years to reduce noise and output data size; indicate number of years to combine here\n",
    "\n",
    "\n",
    "# Inputs below are unlikely to need to be modified unless we change other pieces of the pipeline\n",
    "INPUT_DIR  = f'FILEPATH'\n",
    "HIV_RR_FILE = os.path.join(INPUT_DIR, 'ints_hiv_rr_estimates.csv')\n",
    "CFR_FILE = os.path.join(INPUT_DIR, 'cfr_estimates.csv')\n",
    "\n",
    "DEMOG_VARS = ['location_id', 'year_id', 'age_group_id', 'sex_id']\n",
    "\n",
    "if LEVEL_3 == 'ints':\n",
    "    BUNDLE = 3023\n",
    "    CID = 959\n",
    "elif LEVEL_3 == 'intest':\n",
    "    BUNDLE = 555\n",
    "    CID = [319, 320]\n",
    "else:\n",
    "    sys.exit('Value of LEVEL_3 must be either \"ints\" or \"intest\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80d35203-ff01-4d69-87eb-7681524e2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the levels of the demographic variables & age group metadata\n",
    "demog = get_demographics('epi', release_id = RELEASE)\n",
    "gbd_years = list(range(np.min(demog['year_id']), np.max(demog['year_id'])+1))\n",
    "\n",
    "age_meta = get_age_metadata(release_id = RELEASE)[['age_group_id', 'age_group_years_start', 'age_group_years_end']]\n",
    "age_meta.columns = ['age_group_id', 'age_start', 'age_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddce043f-2d79-4e3a-b97c-3fc9b84ed72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to use data from 'data-rich' countries.  We can pull that list as locs with parent_id 44640 in location_set 43\n",
    "dr_loc_meta = get_location_metadata(43, release_id = RELEASE)\n",
    "dr_locs = dr_loc_meta.loc[dr_loc_meta.parent_id == 44640, ]['location_id'].tolist()\n",
    "\n",
    "# Pull in location metadata to get names, region info, etc. \n",
    "# note: these columns don't contain useful info with location_set 43, so need this seperate pull\n",
    "loc_meta = get_location_metadata(35, release_id = RELEASE)\n",
    "loc_meta = loc_meta[['location_id', 'location_name', 'is_estimate', 'region_id', 'region_name', 'super_region_name', 'super_region_id', 'ihme_loc_id', 'level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e68ff188-d77f-4ac9-aff5-759a4f5e4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the mortality data from the CoD database for only data-rich locations\n",
    "cod_data = get_cod_data(cause_id = CID, release_id = RELEASE, location_id = dr_locs, year_id = gbd_years)\n",
    "\n",
    "# Keep only rows with vital-registration data (data_type 9), with sample of at least 10, and exclude aggregate age groups (22 and 27)\n",
    "cod_data = cod_data[(cod_data['data_type_id'] == 9) & (cod_data['sample_size'] >= 10) & (~cod_data['age_group_id'].isin([22, 27]))]\n",
    "\n",
    "# Keep only needed columns \n",
    "cod_data = cod_data[DEMOG_VARS + ['pop', 'env', 'cf', 'sample_size', 'source_id', 'nid', 'cause_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acab5096-b94f-4e8e-8726-95e6770c738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're only going to keep data from locations that reported cause-specific deaths for at least 5 years\n",
    "# First, find the number of years of data for each location\n",
    "n_years_by_loc = cod_data[['location_id', 'year_id', 'cause_id']].drop_duplicates()[['location_id', 'cause_id']].value_counts()\n",
    "n_years_by_loc = n_years_by_loc.to_frame().reset_index()\n",
    "n_years_by_loc.columns = ['location_id', 'cause_id', 'n_years']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdd96b59-f142-48dd-8899-3d29fea01d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the number of years of data by location with cod_data, and drop locations with < 5 years\n",
    "cod_data = cod_data.merge(n_years_by_loc, on = ['location_id', 'cause_id'], how = 'left')\n",
    "cod_data = cod_data[(cod_data['n_years'] >= 5)]\n",
    "\n",
    "# Calculate the number of deaths as the product of the cause-fraction and total mortality envelop\n",
    "cod_data['deaths'] = cod_data['cf'] * cod_data['env']\n",
    "\n",
    "# Round year to the nearest 5, and collapse variables by rounded year\n",
    "cod_data['year_bin'] = ((cod_data['year_id'] / YEAR_BIN_WIDTH).round(0) * YEAR_BIN_WIDTH).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c572e576-1451-4be4-8fe3-5ee9a9bc074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse to aggregate like data points within each 5-year interval \n",
    "# (we don't need annual resolution, and this aggregation reduces file size, \n",
    "#  speeds up model runtimes, & reduces noise by increasing counts)\n",
    "cod_data = cod_data.groupby(['location_id', 'year_bin', 'age_group_id', 'sex_id', 'source_id', 'cause_id']) \\\n",
    "    .agg(pop = ('pop', 'sum'), \n",
    "         env = ('env', 'sum'), \n",
    "         sample_size = ('sample_size', 'sum'), \n",
    "         deaths = ('deaths', 'sum'),\n",
    "         year_start = ('year_id', 'min'),\n",
    "         year_end = ('year_id', 'max'),\n",
    "         year_id = ('year_id', lambda year_id: year_id.mean().astype(int))).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b3ffac1-5660-434c-93fa-678023438e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pull draws of the cause-fraction with uncertainty.  Obvious approach is with random binomial, but given that we\n",
    "# have fractional deaths (due to redistribution) that won't work.  Instead, we'll treat the proportions as being \n",
    "# logit-normal distributed, and estimate the SE of logit-CF from the CI of the proportion.  Using the Wilson method\n",
    "# for binomial CIs produces CI bounds that are symmetric about the point in logit space, which fits with our assumption\n",
    "# of logit-normality.  Calculating here with other cod data managment, but will use when we pull draws further down. \n",
    "cod_data['cause_fraction'] = cod_data['deaths'] / cod_data['env']\n",
    "cause_fraction_bounds = proportion_confint(cod_data['cause_fraction'] * cod_data['sample_size'], cod_data['sample_size'], method = 'wilson')\n",
    "\n",
    "cod_data['cause_fraction_lower'] = cause_fraction_bounds[0]\n",
    "cod_data['cause_fraction_upper'] = cause_fraction_bounds[1]\n",
    "\n",
    "cod_data['cause_fraction_logit_se'] = np.maximum((logit(cod_data['cause_fraction_upper']) - logit(cod_data['cause_fraction'])), \n",
    "                                                 (logit(cod_data['cause_fraction']) - logit(cod_data['cause_fraction_lower'])))\n",
    "cod_data['cause_fraction_logit_se'] =  cod_data['cause_fraction_logit_se'] / norm.ppf(0.975)\n",
    "\n",
    "\n",
    "# Reshape to wide on cause_id (we're going to do draw-based estimation row-wise and need typhoid and paratyphoid on same row to do so)\n",
    "cod_data = cod_data.pivot(index = ['location_id', 'age_group_id', 'sex_id', 'year_start', 'year_end', 'year_id', 'source_id'], \n",
    "                          columns = 'cause_id', \n",
    "                          values = ['deaths', 'env', 'pop', 'cause_fraction', 'cause_fraction_lower', 'cause_fraction_upper', 'cause_fraction_logit_se'])\n",
    "cod_data.reset_index(inplace = True)\n",
    "cod_data.columns = cod_data.columns.map(lambda x: ' '.join([str(level) for level in x]).strip().replace(\" \", \"_\"))\n",
    "\n",
    "    \n",
    "# Merge location metadata into the cod data\n",
    "cod_data = cod_data.merge(loc_meta, on = 'location_id', how = 'left') \n",
    "\n",
    "# Merge age metadata into the cod data\n",
    "cod_data = cod_data.merge(age_meta, on = 'age_group_id', how = 'left') \n",
    "\n",
    "\n",
    "# Create a dictionary that contains all the unique values of the demographic variables in the cod data\n",
    "# We'll use these values to restrict dimensions of other database pulls\n",
    "cd_dims = {}\n",
    "for dim in DEMOG_VARS:\n",
    "    cd_dims[dim] = cod_data[dim].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81946c-0a92-40a9-907a-bf0d947e4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LEVEL_3 == 'intest':\n",
    "    loc_meta = get_location_metadata(35, release_id = RELEASE)\n",
    "    loc_meta = loc_meta[loc_meta['location_id'].isin(cod_data['location_id'].unique())]\n",
    "    loc_meta['country_id'] =  loc_meta['path_to_top_parent'].str.split(',').str.get(3).astype(int)\n",
    "    loc_meta = loc_meta[['location_id', 'country_id']]\n",
    "\n",
    "    # Connect to the shared database, open the cursor, and execute the SQL query\n",
    "    db = pymysql.connect(host = \"ADDRESS\", user = \"USERNAME\", password = \"PASSWORD\", database = \"DATABASE\") \n",
    "\n",
    "    # Execute the SQL query and fetch all matching data\n",
    "    with db:\n",
    "        with db.cursor() as cursor:\n",
    "            cursor.execute(f\"SELECT location_id, location_metadata_value AS income, location_metadata_version_id \\\n",
    "                           FROM location_metadata_history WHERE location_metadata_type_id = 12 AND location_id IN {tuple(loc_meta['country_id'].unique())}\")\n",
    "            covar_data = pd.DataFrame(cursor.fetchall(), columns = ['country_id', 'income', 'version'])\n",
    "\n",
    "        # The query returns multiple verisons -- keep only the most recent\n",
    "        income = covar_data.loc[covar_data.groupby('country_id')['version'].idxmax()]  \n",
    "\n",
    "    # Need to replace all spaces in income category with underscores to match file names\n",
    "    income['income'] = income['income'].replace(\" \", \"_\", regex = True).replace(\",\", \"\", regex = True)\n",
    "    income = income.drop(columns = 'version')\n",
    "\n",
    "    # Merge income and location metadata to get income for each GBD location with CoD data\n",
    "    income = loc_meta.merge(income, on = 'country_id', how = 'left')\n",
    "    income.loc[income['location_id'].isin([8, 369, 374, 413]), 'income'] = \"Upper_middle_income\"\n",
    "    income.loc[income['location_id'].isin([320]), 'income'] = \"High_income_nonOECD\"\n",
    "\n",
    "\n",
    "\n",
    "    # Read in the correct file and return the df\n",
    "    cf_draws = []\n",
    "    for income_level in income['income'].unique():\n",
    "        tmp = pd.read_csv(os.path.join(INPUT_DIR, f'cfDrawsByIncomeAndAge_{income_level}.csv'))\n",
    "        tmp['income'] = income_level\n",
    "        cf_draws.append(tmp)\n",
    "\n",
    "    cf_draws = pd.concat(cf_draws, ignore_index = True)\n",
    "\n",
    "    cf_draws['cause_id'] = np.where(cf_draws['cause'] == 'paratyphoid', 320,\n",
    "                               np.where(cf_draws['cause'] == 'typhoid', 319, np.nan)).astype(int)\n",
    "\n",
    "    cf_draws = income.merge(cf_draws, on = 'income', how = 'outer')\n",
    "    cf_draws = cf_draws.drop(columns = ['cause', 'country_id', 'income'])\n",
    " \n",
    "    # Sample N_DRAWS from draws in input data\n",
    "    cf_draws['draw_num'] = cf_draws['draw'].str.split('_').str.get(1).astype(int)\n",
    "    keep_draws = random.sample(range(cf_draws['draw_num'].max() + 1), N_DRAWS)\n",
    "    cf_draws = cf_draws.loc[cf_draws['draw_num'].isin(keep_draws)]\n",
    "\n",
    "    cfr = cf_draws.groupby(['location_id', 'age_group_id', 'cause_id']).agg({'case_fatality': ['mean', 'std']}).reset_index()\n",
    "    cfr.columns = ['location_id', 'age_group_id', 'cause_id', 'cfr_mean', 'cfr_se']\n",
    "    cfr = cfr.pivot(index = ['location_id', 'age_group_id'], columns = 'cause_id',\n",
    "                    values = ['cfr_mean', 'cfr_se']).reset_index()\n",
    "\n",
    "    cfr.columns = cfr.columns.map(lambda x: ' '.join([str(level) for level in x]).strip().replace(\" \", \"_\"))\n",
    "\n",
    "    cod_data = cod_data.merge(cfr, on = ['location_id', 'age_group_id'], how = 'left')\n",
    "\n",
    "        \n",
    "\n",
    "    # We have very small fractional counts (rather than integers), and frequent zeros.\n",
    "    # This creates problems for common approaches to creating draws (e.g. random binomial functions require integers,\n",
    "    # log- and logit-normal distributions break with zeros, etc.).\n",
    "    # With that, we're going to use closed-form solutions to propagate uncertainty, instead of draws.  \n",
    "    # This has the additional advantage of running far more quickly.  That said, the math makes for some\n",
    "    # clunky code, and to deal with assymmetric UIs, we're treating the SE of upper bounds and lower bounds\n",
    "    # as distinct.  The equations are:\n",
    "    # se_product = np.sqrt((y_mean**2 * x_se**2) + (x_mean**2 * y_se**2))\n",
    "    # se_quotient = np.sqrt((x_se / x_mean)**2 + (y_se / y_mean)**2) * abs(x_mean / y_mean)\n",
    "    # se_sum = np.sqrt(x_se**2 + y_se**2)\n",
    "\n",
    "    for cid in [319, 320]:\n",
    "        cod_data[f'deaths_lower_se_{cid}'] = cod_data[f'env_{cid}'] * (cod_data[f'cause_fraction_{cid}'] - cod_data[f'cause_fraction_lower_{cid}']) / norm.ppf(0.975)\n",
    "        cod_data[f'deaths_upper_se_{cid}'] = cod_data[f'env_{cid}'] * (cod_data[f'cause_fraction_upper_{cid}'] - cod_data[f'cause_fraction_{cid}']) / norm.ppf(0.975)\n",
    "\n",
    "        cod_data[f'cases_point_{cid}'] = cod_data[f'deaths_{cid}'] / cod_data[f'cfr_mean_{cid}']\n",
    "        cod_data[f'cases_lower_se_{cid}'] = np.sqrt((cod_data[f'deaths_lower_se_{cid}'] / cod_data[f'deaths_{cid}'])**2 \\\n",
    "                                                    + (cod_data[f'cfr_se_{cid}'] / cod_data[f'cfr_mean_{cid}'])**2) \\\n",
    "                                                    * abs(cod_data[f'deaths_{cid}'] / cod_data[f'cfr_mean_{cid}']) \n",
    "\n",
    "        cod_data[f'cases_upper_se_{cid}'] = np.sqrt((cod_data[f'deaths_upper_se_{cid}'] / cod_data[f'deaths_{cid}'])**2 \\\n",
    "                                                  + (cod_data[f'cfr_se_{cid}'] / cod_data[f'cfr_mean_{cid}'])**2) \\\n",
    "                                                  * abs(cod_data[f'deaths_{cid}'] / cod_data[f'cfr_mean_{cid}']) \n",
    "\n",
    "\n",
    "    # Estimate total (typhoid + paratyphoid) cases and UIs\n",
    "    cod_data['cases_point'] = cod_data['cases_point_319'] + cod_data['cases_point_320'] \n",
    "\n",
    "    cod_data['cases_lower_se'] = np.sqrt(cod_data['cases_lower_se_319']**2 + cod_data['cases_lower_se_320']**2)\n",
    "    cod_data['cases_upper_se'] = np.sqrt(cod_data['cases_upper_se_319']**2 + cod_data['cases_upper_se_320']**2)\n",
    "    \n",
    "    # If either typhoid or paratyphoid are zero pull the UIs from the non-missing cause \n",
    "    # (ie. if there are no paratyphoid cases then total cases are just equal to typhoid) \n",
    "    cod_data.loc[cod_data['cases_point_319']==0, 'cases_lower_se'] = cod_data['cases_lower_se_320']\n",
    "    cod_data.loc[cod_data['cases_point_320']==0, 'cases_lower_se'] = cod_data['cases_lower_se_319']\n",
    "    cod_data.loc[cod_data['cases_point_319']==0, 'cases_upper_se'] = cod_data['cases_upper_se_320']\n",
    "    cod_data.loc[cod_data['cases_point_320']==0, 'cases_upper_se'] = cod_data['cases_upper_se_319']\n",
    "\n",
    "    cod_data['cases_lower'] = cod_data['cases_point'] - cod_data['cases_lower_se'] * norm.ppf(0.975)\n",
    "    cod_data['cases_upper'] = cod_data['cases_point'] + cod_data['cases_upper_se'] * norm.ppf(0.975)\n",
    "    \n",
    "\n",
    "\n",
    "    cod_data.loc[cod_data['cases_lower']<0, 'cases_lower'] = 0\n",
    "    cod_data.loc[cod_data['cases_upper'] > cod_data['pop_319'], 'cases_upper'] = cod_data['pop_319']\n",
    "    \n",
    "    cod_data['incidence_point'] = cod_data['cases_point'] / cod_data[['pop_319', 'pop_320']].mean(axis = 1)\n",
    "    cod_data['incidence_lower'] = cod_data['cases_lower'] / cod_data[['pop_319', 'pop_320']].mean(axis = 1)\n",
    "    cod_data.loc[cod_data['incidence_point']==0, 'incidence_lower'] = 0\n",
    "    cod_data['incidence_upper'] = cod_data['cases_upper'] / cod_data[['pop_319', 'pop_320']].mean(axis = 1)\n",
    "    cod_data.loc[cod_data['incidence_point']==1, 'incidence_upper'] = 1\n",
    "    \n",
    "    # Estimate proportion of all cases that are paratyphoid and UIs\n",
    "    cod_data['pr_para_point'] = cod_data['cases_point_320'] / cod_data['cases_point']\n",
    "\n",
    "    cod_data['pr_para_lower_se'] = np.sqrt((cod_data['cases_lower_se_320'] / cod_data['cases_point_320'])**2 + \\\n",
    "                                           (cod_data['cases_lower_se'] / cod_data['cases_point'])**2) * \\\n",
    "                                            abs(cod_data['cases_point_320'] / cod_data['cases_point'])\n",
    "    \n",
    "    cod_data['pr_para_upper_se'] = np.sqrt((cod_data['cases_upper_se_320'] / cod_data['cases_point_320'])**2 + \\\n",
    "                                           (cod_data['cases_upper_se'] / cod_data['cases_point'])**2) * \\\n",
    "                                            abs(cod_data['cases_point_320'] / cod_data['cases_point'])\n",
    "\n",
    "    pr_para_bounds = proportion_confint(cod_data['cases_point_320'], cod_data['cases_point'], method = 'wilson')\n",
    "    cod_data['pr_para_lower'] = pr_para_bounds[0]\n",
    "    cod_data['pr_para_upper'] = pr_para_bounds[1]\n",
    "\n",
    "    cod_data.loc[cod_data['pr_para_lower'] < 0, 'pr_para_lower'] = 0\n",
    "    cod_data.loc[cod_data['pr_para_upper'] > 1, 'pr_para_upper'] = 1 \n",
    "\n",
    "    \n",
    "    # Reshape to long, clean up variable names and add a bundle_id column to prep for bundle upload\n",
    "    inc_tmp = cod_data[['location_id', 'level', 'age_start', 'age_end', 'sex_id', 'year_start', 'year_end', 'incidence_point', 'incidence_lower', 'incidence_upper', 'cases_point_319', 'cases_point_320']]\n",
    "    inc_tmp.columns = ['location_id', 'level', 'age_start', 'age_end', 'sex_id', 'year_start', 'year_end', 'mean', 'lower', 'upper', 'typhoid_cases', 'paratyphoid_cases']\n",
    "    inc_tmp = inc_tmp.query('typhoid_cases.notnull() & paratyphoid_cases.notnull()')\n",
    "    inc_tmp['bundle_id'] = 556\n",
    "    inc_tmp['measure'] = 'incidence'\n",
    "\n",
    "\n",
    "    prp_tmp = cod_data[['location_id', 'level', 'age_start', 'age_end', 'sex_id', 'year_start', 'year_end', 'pr_para_point', 'pr_para_lower', 'pr_para_upper', 'cases_point_319', 'cases_point_320']]\n",
    "    prp_tmp.columns = ['location_id', 'level', 'age_start', 'age_end', 'sex_id', 'year_start', 'year_end', 'mean', 'lower', 'upper', 'typhoid_cases', 'paratyphoid_cases']\n",
    "    prp_tmp = prp_tmp.query('(typhoid_cases > 0 | paratyphoid_cases > 0) & typhoid_cases.notnull() & paratyphoid_cases.notnull()')\n",
    "    prp_tmp['bundle_id'] = 18\n",
    "    prp_tmp['measure'] = 'proportion'\n",
    "\n",
    "\n",
    "    for_bundle = pd.concat([inc_tmp, prp_tmp])\n",
    "\n",
    "    for_bundle.loc[for_bundle['mean']==0, 'lower'] = 0\n",
    "    for_bundle.loc[for_bundle['mean']==1, 'upper'] = 1\n",
    "\n",
    "    # Read in the case-fatality estimates & retain only rows for non-HIV attribuable \n",
    "    # (since HIV-attributable would have been coded as HIV deaths)\n",
    "    case_fatality = pd.read_csv(CFR_FILE).query('est_pr_hiv == 0').drop(columns = ['est_pr_hiv'])\n",
    "\n",
    "    # Merge case-fatality estimates into cod data\n",
    "    cod_data = cod_data.merge(case_fatality, on = ['location_id', 'year_id', 'age_group_id'], how = 'left')\n",
    "\n",
    "    # Read in the estimates of HIV prevalence, keep needed columns and clean up variable names\n",
    "    hiv_prev = get_covariate_estimates(covariate_id = 49, \n",
    "                                       location_id = cd_dims['location_id'],\n",
    "                                       year_id = cd_dims['year_id'],\n",
    "                                       age_group_id = cd_dims['age_group_id'],\n",
    "                                       release_id = RELEASE)\n",
    "    \n",
    "    hiv_prev = hiv_prev[DEMOG_VARS + ['mean_value', 'lower_value', 'upper_value']]\n",
    "\n",
    "    hiv_prev = hiv_prev.rename(columns = {'mean_value':'hiv_prev_mean', \n",
    "                                          'lower_value':'hiv_prev_lower', \n",
    "                                          'upper_value':'hiv_prev_upper'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9d75f-3c4b-4196-a6bd-a36da0fed055",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_data = cod_data.merge(hiv_prev, on = DEMOG_VARS, how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "265a8000-0852-4c60-8984-3dfbe10d6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LEVEL_3 == 'ints':\n",
    "    # Read in the case-fatality estimates & retain only rows for non-HIV attribuable \n",
    "    # (since HIV-attributable would have been coded as HIV deaths)\n",
    "    case_fatality = pd.read_csv(CFR_FILE).query('est_pr_hiv == 0').drop(columns = ['est_pr_hiv'])\n",
    "\n",
    "    # Merge case-fatality estimates into cod data\n",
    "    cod_data = cod_data.merge(case_fatality, on = ['location_id', 'year_id', 'age_group_id'], how = 'left')\n",
    "\n",
    "    # Read in the estimates of HIV prevalence, keep needed columns and clean up variable names\n",
    "    hiv_prev = get_covariate_estimates(covariate_id = 49, \n",
    "                                       location_id = cd_dims['location_id'],\n",
    "                                       year_id = cd_dims['year_id'],\n",
    "                                       age_group_id = cd_dims['age_group_id'],\n",
    "                                       release_id = RELEASE)\n",
    "    \n",
    "    hiv_prev = hiv_prev[DEMOG_VARS + ['mean_value', 'lower_value', 'upper_value']]\n",
    "\n",
    "    hiv_prev = hiv_prev.rename(columns = {'mean_value':'hiv_prev_mean', \n",
    "                                          'lower_value':'hiv_prev_lower', \n",
    "                                          'upper_value':'hiv_prev_upper'})\n",
    "    \n",
    "    # Merge HIV prevalence estimates into cod_data data frame\n",
    "    cod_data = cod_data.merge(hiv_prev, on = DEMOG_VARS, how = 'left')\n",
    "    \n",
    "    # Use method of moments to determine the parameters of a gamma distribution that best fit the mean and uncertainty of HIV prevalence estimates\n",
    "    # We'll use alpha and beta to pull draws of HIV prevalence from a gamma distribution in our draw-level calculations below\n",
    "    cod_data['sigma'] = (cod_data['hiv_prev_upper'] - cod_data['hiv_prev_lower']) / ( 2 * norm.ppf(0.975))\n",
    "    cod_data.loc[cod_data['sigma']==0, 'sigma'] = cod_data.query('hiv_prev_mean != 0 & sigma>0')['sigma'].min()\n",
    "    \n",
    "    cod_data['alpha'] = cod_data['hiv_prev_mean'] * (cod_data['hiv_prev_mean'] - cod_data['hiv_prev_mean']**2 - cod_data['sigma']**2) / cod_data['sigma']**2\n",
    "    cod_data.loc[cod_data['alpha']<0, 'alpha'] = 0\n",
    "\n",
    "    cod_data['beta'] = cod_data['alpha'] * (1 - cod_data['hiv_prev_mean']) / cod_data['hiv_prev_mean']\n",
    "    \n",
    "    # Read in the estimates of HIV-iNTS relative risks and merge into the cod_data\n",
    "    hiv_rr = pd.read_csv(HIV_RR_FILE)[DEMOG_VARS + ['log_hiv_excess_risk', 'log_hiv_excess_risk_se']]\n",
    "    cod_data = cod_data.merge(hiv_rr, on = DEMOG_VARS, how = 'left')\n",
    "    \n",
    "    # Calculate point estimate of cases (no draws, no uncertainty) \n",
    "    cod_data['cfr_point'] = expit(cod_data['logit_pred'])\n",
    "    cod_data['hiv_paf_point'] = (cod_data['hiv_prev_mean'] * np.exp(cod_data['log_hiv_excess_risk'])) / (cod_data['hiv_prev_mean'] * np.exp(cod_data['log_hiv_excess_risk']) + 1)  \n",
    "    cod_data['incidence_point'] = ((cod_data['deaths'] / expit(cod_data['logit_pred'])) / (1 - cod_data['hiv_paf_point'])) / cod_data['pop']\n",
    "    \n",
    "    # Apply calc_ints_incidence_ui function to all rows to estimate 95% UI bounds for the incidence estimate\n",
    "    incidence_ui = cod_data.apply(calc_ui, axis = 1)\n",
    "    incidence_ui = pd.DataFrame(incidence_ui.tolist(), columns = ['incidence_mean', 'incidence_lower', 'incidence_upper'])\n",
    "\n",
    "    # Concat the incidence UI columns onto the cod_data data frame\n",
    "    cod_data = pd.concat([cod_data.reset_index(), incidence_ui.reset_index(drop = True)], axis = 1)\n",
    "    \n",
    "    # Given very small counts we frequently get wild draws that make for impossible UIs (e.g. upper < mean)\n",
    "    # We'll fix this by extracting the width of the UI segment and applying it to the directly calculated\n",
    "    # point estimate (rather than the mean of the draws) all in logit space\n",
    "    cod_data['incidence_lower'] = expit(logit(cod_data['incidence_point']) - (logit(cod_data['incidence_mean']) - logit(cod_data['incidence_lower'])))\n",
    "    cod_data['incidence_upper'] = expit(logit(cod_data['incidence_point']) + (logit(cod_data['incidence_upper']) - logit(cod_data['incidence_mean'])))\n",
    "    \n",
    "    for_bundle = cod_data[['location_id', 'level', 'age_start', 'age_end', 'sex_id', 'year_start', 'year_end', 'incidence_point', 'incidence_lower', 'incidence_upper']]\n",
    "    for_bundle.columns = ['location_id', 'level', 'age_start', 'age_end', 'sex_id', 'year_start', 'year_end', 'mean', 'lower', 'upper']\n",
    "    for_bundle['bundle_id'] = BUNDLE\n",
    "    for_bundle['measure'] = 'incidence'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f80bd289-6ec5-42f1-bc36-042e9abd16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to prep things for upload\n",
    "\n",
    "for_bundle.loc[for_bundle['sex_id'] == 1, 'sex'] = 'Male'\n",
    "for_bundle.loc[for_bundle['sex_id'] == 2, 'sex'] = 'Female'\n",
    "for_bundle['source_type'] = 'Vital registration - national' \n",
    "for_bundle['nid'] = 292827\n",
    "for_bundle['unit_type'] = 'Person'\n",
    "for_bundle['unit_value_as_published'] = 1\n",
    "for_bundle['measure_adjustment'] = 0\n",
    "for_bundle['uncertainty_type_value'] = 95\n",
    "for_bundle['urbanicity_type'] = 'Mixed/both'\n",
    "for_bundle.loc[for_bundle['level'] == 3, 'representative_name'] = 'Nationally representative only'\n",
    "for_bundle.loc[for_bundle['level'] > 3, 'representative_name'] = 'Representative for subnational location only'\n",
    "for_bundle['recall_type'] = 'Point'\n",
    "for_bundle['extractor'] = 'stanaway'\n",
    "for_bundle['is_outlier'] = 0\n",
    "for_bundle['cv_diag_mixed'] = 0\n",
    "for_bundle['cv_passive'] = 0\n",
    "for_bundle['smaller_site_unit'] = 0\n",
    "for_bundle['sex_issue'] = 0\n",
    "for_bundle['year_issue'] = 0\n",
    "for_bundle['age_issue'] = 0\n",
    "for_bundle['age_demographer'] = 0\n",
    "for_bundle['measure_issue'] = 0\n",
    "for_bundle['response_rate'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cad9b690-bf12-4884-872e-247d2e0c28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_bundle.to_csv(os.path.join(INPUT_DIR, 'vr_data_for_dismod_bundle.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
