"""

!~!~!~!

This is not currently in production, but if we want to make any changes we should
remove the existing cluster and model pre format scripts and just use this one
!~!~!~!


Created on Tue Jan 17 12:07:25 2017
Taking another look at this in July 2019
Will have a meeting in early August to decide outcome

No definable outcome decided
MNCH is outliering most of this data

If someone looks into this again this a basically a copy of the
UK..cluster.py file with some modifications


@author: Dillon Sylte & Chris Castle

formatting UK UTLA data
"""

import multiprocessing
import time
import warnings

import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from db_queries import get_population
from db_tools.ezfuncs import query

from inpatient.Clinical_Runs.clean_final_bundle import (
    get_asfr,
    get_maternal_bundles,
    merge_asfr,
)
from crosscutting_functions import *
from inpatient.Mapping.clinical_mapping import get_clinical_process_data

assert False, "Not ready for showtime, gotta remove the maternal stuff i added"
prep_maternal_sample = True


def adjust_ages(df, age_df):
    # change ages in data
    df["age_end"] = df["age_end"] + 1
    df.loc[df["age_start"] == 0, "age_end"] = 1
    df.loc[df["age_start"] == 90, "age_end"] = 125
    for a_e in df.age_end.unique():
        assert a_e in age_df.age_end.unique(), "uh oh"
    return df


def model_by_cause(cause):
    """Takes a dataframe and a single icd code, fits a model and makes predicted rate"""
    tmp = both[both["cause_code"] == cause].copy()  # `both` is a global var
    assert tmp["cause_code"].unique().size == 1, "More than 1 ICD code"

    for s in [1, 2]:
        # create the mask
        mask = (tmp["cause_code"] == cause) & (tmp["sex_id"] == s)
        if tmp[mask].log_rate.isnull().sum() == tmp[mask].shape[0]:
            print("there's no data")
            continue
        # our formula for predictions
        formula = "log_rate ~ C(age_start) + C(location_id)"
        # fit the model
        fit = smf.ols(formula, data=tmp[mask]).fit()
        # exponentiate the predicted values
        tmp.loc[mask, "preds"] = np.exp(fit.predict(tmp[mask]))

    return tmp


base = "/ihme/hospital/clinical_data/migration_from_j/UK_HOSPITAL_STATISTICS"

# read in utla data
utla = pd.read_csv("{}/raw_inputs/utla_data_2001_2014.csv".format(base))

# read in regional data
# reg_o = pd.read_table(root + r"/DATA/GBR/ENGLAND_HOSPITAL_EPISODE_STATISTICS/"
#                    r"GBR_ENGLAND_HES_2001_2012_INPATIENT_BY_"
#                    r"AGE_SEX_REGION_CAUSE_IMD_Y2014M02D25.TXT")
# new regional data
reg = pd.read_csv("{}/raw_inputs/regional_data.csv".format(base))

# create icd code and year specific inpatient weights
weights = pd.read_excel("{}/raw_inputs/inpatient_weights.xlsx".format(base))

# make sure all ICD codes match
assert not set(weights.DiagCode3.unique()).symmetric_difference(
    utla.DiagCode3.unique()
), "there's an icd diff"

# keep the columns we like
weights = weights[["FYEAR", "Inpatient", "DiagCode3", "value"]]
# rename them
weights.rename(columns={"Inpatient": "inpatient_id", "DiagCode3": "cause_code"}, inplace=True)

# create denom and calculate weight
weights["denominator"] = weights.groupby(["FYEAR", "cause_code"])["value"].transform("sum")
# drop day cases
weights = weights[weights["inpatient_id"] == 1]
# create the inpatient weights (multiply raw value by this ratio to get only
# inpatient cases)
weights["weight"] = weights["value"] / weights["denominator"]
assert weights.weight.max() <= 1
# keep only year, icde code and weight
weights = weights[["FYEAR", "cause_code", "weight"]]

# convert region codes to region names
# make dict of code:name pairs
reg_dict = {
    "E12000001": "North East",
    "E12000002": "North West",
    "E12000003": "Yorkshire and The Humber",
    "E12000004": "East Midlands",
    "E12000005": "West Midlands",
    "E12000006": "East of England",
    "E12000007": "London",
    "E12000008": "South East",
    "E12000009": "South West",
}
# map on names
reg["region_name"] = reg["REGION"].map(reg_dict)

# formate ages into age_start and age_end in utla
reg.loc[reg["AgeBand"] == "90+", "AgeBand"] = "9099"
reg.loc[reg["AgeBand"] == "0000", "AgeBand"] = "0001"
reg.dropna(subset=["AgeBand"], inplace=True)

reg["age_start"] = reg["AgeBand"].str[0:2]
reg["age_end"] = reg["AgeBand"].str[2:]
reg["age_start"] = pd.to_numeric(reg["age_start"], errors="raise")
reg["age_end"] = pd.to_numeric(reg["age_end"], errors="raise")

reg.drop(["AgeBand", "REGION"], axis=1, inplace=True)

# create fiscal year for the merge
reg["FYEAR"] = reg["FYEAR"].astype(str)
reg["year_end"] = reg["FYEAR"].str[-2:].astype(np.int)
reg["year_end"] = reg["year_end"] + 2000
reg["year_start"] = reg["year_end"] - 1
reg["fiscal_year"] = (reg["year_start"] + reg["year_end"]) / 2

# rename to sex_id for the merge
reg.rename(
    columns={"SEX": "sex_id", "DiagCode3": "cause_code", "value": "reg_value"},
    inplace=True,
)

# To merge location id onto reg
reg.loc[reg.region_name == "North East", "region_name"] = "North East England"
reg.loc[reg.region_name == "North West", "region_name"] = "North West England"
reg.loc[reg.region_name == "South East", "region_name"] = "South East England"
reg.loc[reg.region_name == "South West", "region_name"] = "South West England"
reg.loc[
    reg.region_name == "Yorkshire and The Humber", "region_name"
] = "Yorkshire and the Humber"
reg.loc[reg.region_name == "London", "region_name"] = "Greater London"

# merge the weights onto regional data
weights["FYEAR"] = weights["FYEAR"].astype(str)
pre = len(reg)
pre_cases = reg["reg_value"].sum()
reg = reg.merge(weights, how="left", on=["FYEAR", "cause_code"])
assert pre == len(reg)
# reg['unweighted_reg_value'] = reg['reg_value']
reg["reg_value"] = reg["reg_value"] * reg["weight"]
reg.drop("weight", axis=1, inplace=True)

### Prep UTLA Data

# Location
# read in utla name map
utla_name_map = pd.read_excel(
    "/home/j/DATA/GBR/"
    r"ENGLAND_HOSPITAL_EPISODE_STATISTICS/"
    r"GBR_ENGLAND_HES_2001_2014_METADATA_"
    r"Y2016M10D10.XLSX",
    sheet_name="England_UTLA_codes",
)
utla_name_map.rename(columns={"Code": "utla_code", "Name": "utla_name"}, inplace=True)

# drop rows where UTLA11 is missing
utla = utla.dropna(subset=["UTLA11"])

# merge on the utla names onto utla data
utla.rename(columns={"UTLA11": "utla_code"}, inplace=True)
utla = utla.merge(utla_name_map, how="left", on="utla_code")


# formate ages into age_start and age_end in utla
utla.loc[utla["AgeBand"] == "90+", "AgeBand"] = "9099"
utla.loc[utla["AgeBand"] == "0000", "AgeBand"] = "0001"
utla.dropna(subset=["AgeBand"], inplace=True)

utla["age_start"] = utla["AgeBand"].str[0:2]
utla["age_end"] = utla["AgeBand"].str[2:]
utla["age_start"] = pd.to_numeric(utla["age_start"], errors="raise")
utla["age_end"] = pd.to_numeric(utla["age_end"], errors="raise")

utla.drop("AgeBand", axis=1, inplace=True)


# create fiscal year for the merge
utla["FYEAR"] = utla["FYEAR"].astype(str)
utla["year_end"] = utla["FYEAR"].str[-2:].astype(np.int)
utla["year_end"] = utla["year_end"] + 2000
utla["year_start"] = utla["year_end"] - 1
utla["fiscal_year"] = (utla["year_start"] + utla["year_end"]) / 2

# rename to sex_id for the merge
utla.rename(columns={"SEX": "sex_id", "DiagCode3": "cause_code"}, inplace=True)

# merge the weights onto UTLA data
pre = len(utla)
utla = utla.merge(weights, how="left", on=["FYEAR", "cause_code"])
assert len(utla) == pre
# utla['raw_value'] = utla['value']
# coerce value column to a float creating NaNs where the supressed values were
supressed = utla[utla.value == "*"].shape[0]
utla["value"] = pd.to_numeric(utla["value"], errors="coerce")
assert utla[utla.value.isnull()].shape[0] == supressed, "Some supressed values were lost"
utla["value"] = utla["value"] * utla["weight"]
utla.drop("weight", axis=1, inplace=True)

# confirm ages and years match between utla and regional data
assert set(utla.age_start.unique()).symmetric_difference(set(reg.age_start.unique())) == set()
assert set(utla.age_end.unique()).symmetric_difference(set(reg.age_end.unique())) == set()
assert (
    set(reg.fiscal_year.unique()).symmetric_difference(set(utla.fiscal_year.unique())) == set()
)
assert (
    set(reg.cause_code.unique()).symmetric_difference(set(utla.cause_code.unique())) == set()
)

# read in population
# warnings.warn("We're getting currently getting population from a flat file")
badpop = pd.read_csv("/home/j/temp/hospital/2016/data/uk_population.csv")
pop = get_population(
    decomp_step="step2",
    age_group_id=badpop.age_group_id.unique().tolist(),
    year_id=badpop.year_id.unique().tolist(),
    sex_id=[1, 2, 3],
    location_id=badpop.location_id.unique().tolist(),
)
age_df = hosp_prep.get_hospital_age_groups()
pre = pop.shape[0]
pop = pop.merge(age_df, how="left", on="age_group_id")
assert pre == pop.shape[0]

# add terminal ages for regional data
# for_reg_pop = pop[pop.age_start >= 85].copy()
# for_reg_pop['age_start'] = 85
# for_reg_pop['age_end'] = 99
# for_reg_pop = for_reg_pop.groupby(['location_id', 'sex_id', 'age_start', 'age_end',
#                    'year_id']).agg({'population': 'sum'}).\
#                    reset_index()
# age group 160 doesn't exist in pop
# pop.loc[pop.age_group_id == 160, ['age_start', 'age_end']] = [85, 99]
# pop = pd.concat([pop, for_reg_pop])

pop.rename(columns={"year_id": "year_start"}, inplace=True)
pop.drop(["age_group_id"], axis=1, inplace=True)

pop_sum = pop[pop.age_start.notnull()].population.sum()


# uk utla and reg data has populations that end at 90+.. which isn't in the db
# sum 90-94 and 95+
pop.loc[pop.age_start > 89, ["age_start", "age_end"]] = [90, 125]

pop = (
    pop.groupby(["location_id", "sex_id", "age_start", "age_end", "year_start"])
    .agg({"population": "sum"})
    .reset_index()
)
assert round(pop.population.sum(), 3) == round(pop_sum, 3), "groupby has changed pop counts"

# utla and uk regional data uses fiscal years, so let's take the average of
# every 2 years to make a 'fiscal' population
pop["start_pop"] = pop["population"] / 2
pop["end_pop"] = pop["population"] / 2
pop["year_start"] = pop["year_start"] + 0.5
pop["year_end"] = pop["year_start"] + 1

pop.drop(["population"], axis=1, inplace=True)
# reshape long to duplicate half counts
pop = (
    pop.set_index(["location_id", "year_start", "year_end", "sex_id", "age_start", "age_end"])
    .stack()
    .reset_index()
)
# y['fiscal_year'] = (y['year_start'] + y['year_end']) / 2

# match year start and year end to the discharge data
pop.loc[pop["level_6"] == "start_pop", "year_end"] = pop.loc[
    pop["level_6"] == "start_pop", "year_start"
]
pop.loc[pop["level_6"] == "end_pop", "year_start"] = pop.loc[
    pop["level_6"] == "end_pop", "year_end"
]
pop.rename(columns={0: "population"}, inplace=True)

# groupby demographics and sum pop to get the average between years
pop = (
    pop.groupby(["location_id", "sex_id", "age_start", "age_end", "year_start", "year_end"])
    .agg({"population": "sum"})
    .reset_index()
)
pop["fiscal_year"] = pop["year_start"]
pop.drop(["year_start", "year_end"], axis=1, inplace=True)


# utla/uk/england location IDs
loc = pd.read_csv("/home/j/temp/hospital/2016/data/uk_location_ids.csv")
loc.loc[
    loc.location_name.str.contains("hackney", case=False), "location_name"
] = "Hackney & City of London"
loc.loc[loc.location_name.str.contains("helens", case=False), "location_name"] = "St. Helens"
assert set(utla.utla_name) - set(loc.location_name) == set()

### Merge location and population onto UTLA and Regional data
utla = adjust_ages(utla, age_df)
reg = adjust_ages(reg, age_df)

# merge location id onto UTLA
pre = utla.shape[0]
utla = utla.merge(loc, how="left", left_on="utla_name", right_on="location_name")
assert pre == utla.shape[0]
assert utla.location_id.isnull().sum() == 0

# merge population onto utla data by location/age/sex/year
pre = utla.shape[0]
utla = utla.merge(
    pop, how="left", on=["location_id", "fiscal_year", "sex_id", "age_start", "age_end"]
)
assert pre == utla.shape[0]


# coerce value column to a float creating NaNs where the supressed values were
# supressed = utla[utla.value == '*'].shape[0]
# utla['value'] = pd.to_numeric(utla['value'], errors='coerce')
# assert utla[utla.value.isnull()].shape[0] == supressed,            "Some supressed values were lost"

pre = reg.shape[0]
reg = reg.merge(loc, how="left", left_on="region_name", right_on="location_name")
assert pre == reg.shape[0]
assert reg.location_id.isnull().sum() == 0

# merge population onto reg data by location/age/sex/year
pre = reg.shape[0]
# reg.loc[reg.age_start == 85, 'age_end'] = 99
# reg_pop = pop[pop.age_end != 89].copy()
reg = reg.merge(
    pop, how="left", on=["location_id", "fiscal_year", "sex_id", "age_start", "age_end"]
)

# verify the population merges, only null populations should be caused by unusual sex ids
assert set(utla[utla.population.isnull()].sex_id).symmetric_difference(set([9, 0])) == set()
assert set(reg[reg.population.isnull()].sex_id).symmetric_difference(set([9, 0])) == set()

reg.rename(columns={"reg_value": "value"}, inplace=True)

backreg = reg.copy()


reg = backreg.copy()


def prep_uk(df, mat_bundles, mat_icg):
    mat = df.query(
        f"cause_code in {mat_icg.cause_code.tolist()} and age_start < 55 and age_start > 9 and sex_id == 2"
    ).copy()
    drops = ["FYEAR", "utla_name", "region_name", "fiscal_year", "location_parent_id"]
    for d in drops:
        if d in mat.columns:
            mat.drop(d, axis=1, inplace=True)

    mat = mat.merge(age_df, how="left", on=["age_start", "age_end"])
    mat["year_start_id"] = mat["year_start"]
    asfr = get_asfr(mat, "step2")
    mat = merge_asfr(mat, asfr)

    mat["live_births"] = mat["population"] * mat["mean_value"]
    mat["population_rate"] = mat["value"] / mat["population"]
    mat["live_birth_rate"] = mat["value"] / mat["live_births"]

    mat2 = mat[mat.value.notnull()].copy()
    mat2.sort_values(["cause_code", "location_id", "year_start", "age_start"], inplace=True)

    # merge on bundle and sum pop and live birth rate
    bmerge = mat_bundles[["icg_id", "bundle_id"]].merge(
        mat_icg[["cause_code", "icg_id"]], how="left", on="icg_id"
    )
    bmerge = bmerge.drop("icg_id", axis=1).drop_duplicates()
    mat2 = mat2.merge(bmerge, how="left", on="cause_code")
    groupers = [
        "age_start",
        "age_end",
        "sex_id",
        "location_id",
        "year_start",
        "population",
        "live_births",
        "bundle_id",
    ]
    mat_bundle = (
        mat2.groupby(groupers)
        .agg({"value": "sum", "population_rate": "sum", "live_birth_rate": "sum"})
        .reset_index()
    )

    mat_bundles = tuple(mat2.bundle_id.unique().tolist())
    bun_map = query(
        f"select bundle_id, bundle_name from bundle.bundle where bundle_id in {mat_bundles}",
        conn_def="epi",
    )
    mat_bundle = mat_bundle.merge(bun_map, how="left", on="bundle_id", validate="m:1")
    mat2 = mat2.merge(bun_map, how="left", on="bundle_id", validate="m:1")

    return mat, mat2, mat_bundle


if prep_maternal_sample:
    mat_bundles = get_clinical_process_data("icg_bundle")
    all_mat_bundles = get_maternal_bundles()
    mat_bundles = mat_bundles.query(f"bundle_id in {all_mat_bundles}")
    mat_bundles = mat_bundles.query("bundle_id != 1010")
    mat_icg = get_clinical_process_data("cause_code_icg")
    mat_icg = mat_icg.query(
        f"icg_id in {tuple(mat_bundles.icg_id.unique())} and code_system_id == 2"
    )

    mat_reg, mat2_reg, mat_bundle_reg = prep_uk(reg.copy(), mat_bundles, mat_icg)

    mat_utla, mat2_utla, mat_bundle_utla = prep_uk(utla.copy(), mat_bundles, mat_icg)

    mat_bundle.drop(["population_rate"], 1).to_excel(
        "/homes/castlc/data/utla_maternal_bundle.xlsx", index=False
    )
    mat2.drop(["population_rate"], 1).to_excel(
        "/homes/castlc/data/utla_maternal_icd.xlsx", index=False
    )

    print("compare to US data")
    us = []
    fpath = "/ihme/hospital/clinical_runs/run_7/estimates/inpatient/final/icg_single_years/final_icg.H5"
    us_iter = pd.read_hdf(fpath, chunksize=1000000)
    for chunk in us_iter:
        chunk = chunk.query("source == 'USA_HCUP_SID'")
        us.append(chunk)
    us = pd.concat(us, sort=False, ignore_index=True)
    us = hosp_prep.group_id_start_end_switcher(us.copy(), remove_cols=False)
    us = us.query("estimate_id == 6")
    locs = query("select location_id, location_name from shared.location", conn_def="epi")
    us = us.merge(locs, how="left", on="location_id")
    us = us.merge(mat_bundles, how="left", on=["icg_id", "icg_name"])
    us["cases"] = us["sample_size"] * us["mean"]

    ## uhhh ok, let's compare by age, sex, subnat, bundle_id and that's it
    groups = [
        "age_start",
        "age_end",
        "sex_id",
        "location_id",
        "location_name",
        "bundle_id",
        "bundle_name",
    ]
    sum_dict = {"cases": "sum", "sample_size": "sum"}
    usf = us.copy()
    usf = usf.merge(
        regf[["bundle_id", "bundle_name"]].drop_duplicates(), how="left", on="bundle_id"
    )
    regf = mat2_reg.copy()
    regf.rename(columns={"value": "cases", "live_births": "sample_size"}, inplace=True)
    usf = usf.groupby(groups).agg(sum_dict).reset_index()
    regf = regf.groupby(groups).agg(sum_dict).reset_index()

    # rate per 100k
    usf["rate_per_100k"] = (usf["cases"] / usf["sample_size"]) * 1e5
    regf["rate_per_100k"] = (regf["cases"] / regf["sample_size"]) * 1e5

    remove_bundles = ["Fistula", "Maternal abortive outcome"]
    usf[~usf.bundle_name.isin(remove_bundles)].to_csv(
        "/ihme/homes/castlc/data/us_maternal_bundles.csv", index=False
    )
    regf[~regf.bundle_name.isin(remove_bundles)].to_csv(
        "/ihme/homes/castlc/data/gbr_region_bundles.csv", index=False
    )

# maybe write utla and regional level data here for bundle 74
# icd codes ['O20', 'O44', 'O45', 'O46', 'O67', 'O72']


### Final prep for the merge

assert pre == reg.shape[0]
# assert reg.population.isnull().sum() == 0
utla["in_utla_data"] = True
# make log rate
utla["utla_log_rate"] = np.log(utla["value"] / utla["population"])
utla = utla[
    [
        "cause_code",
        "location_id",
        "location_parent_id",
        "population",
        "age_start",
        "age_end",
        "sex_id",
        "fiscal_year",
        "value",
        "utla_log_rate",
        "in_utla_data",
    ]
]

# TODO collapse reg to sum up on icd codes
# think about if this is the right way to do population
reg = reg[
    [
        "cause_code",
        "location_id",
        "age_start",
        "age_end",
        "sex_id",
        "fiscal_year",
        "reg_value",
        "population",
    ]
]
reg = (
    reg.groupby(
        [
            "cause_code",
            "location_id",
            "age_start",
            "age_end",
            "sex_id",
            "fiscal_year",
            "population",
        ]
    )
    .agg({"reg_value": "sum"})
    .reset_index()
)
# make log rate
reg["reg_log_rate"] = np.log(reg["reg_value"] / reg["population"])

# drop fiscal years and ages that aren't in regional data
# utla = utla[utla['fiscal_year'] <= reg['fiscal_year'].max()]

# drop unknown sexes
utla = utla.query("sex_id == 1 | sex_id == 2")
reg = reg.query("sex_id == 1 | sex_id == 2")

# rename loc id to loc parent id so they don't create loc_id_x/y cols
reg.rename(
    columns={"location_id": "location_parent_id", "population": "reg_populuation"},
    inplace=True,
)
utla.rename(columns={"population": "utla_population"}, inplace=True)


### Merge Regional data onto UTLA data
both = utla.merge(
    reg,
    how="left",
    on=[
        "location_parent_id",
        "sex_id",
        "age_start",
        "age_end",
        "fiscal_year",
        "cause_code",
    ],
)
# on the rows where utla data is missing
# fill in the regional values
both["log_rate"] = both["utla_log_rate"]
both.loc[both.log_rate.isnull(), "log_rate"] = both.loc[both.log_rate.isnull(), "reg_log_rate"]

# the weights we got from Andrew don't match every ICD code. There are a few
# cases, about 500 (out of millions) that are lost
both = both[both["reg_value"].notnull()]
# we want every row to have a log rate, either from utla or regional level data
assert both["log_rate"].isnull().sum() == 0
assert both["age_start"].unique().size == 20
assert both["fiscal_year"].unique().size == 14

# write to /ihme
both.to_csv(
    "/ihme/hospital/clinical_data/inputs_from_j/UK_HOSPITAL_STATISTICS/intermediate_prep_and_wide_files/uk_utla_regional_merged_full_years_inp_only.csv",
    index=False,
)

# the regional level data needs to be split to include age start 90
# it's breaking the models so I'm gonna subset that age group out
# both = both[both.age_start < 80]
# also drop 2011, 2012
# both = both[both.fiscal_year < 2011]

# drop the rows that don't match (only 2 rows before 2011)
both = both[~both.log_rate.isnull()]

print(
    "We would need to square the data, then apply age_sex restrictions somehwere around here"
)
print("But wait, how do you apply ICD level age-sex restrictions??")

##################################
# FIT THE LINEAR MODELS
###################################
# this takes about 3 hours to run serially

causes = both.cause_code.unique()
# both = both[both.cause_code.isin(causes)]
both["preds"] = np.nan  # initialize pred col


# write a func using multiprocessing that takes just ICD code as an arg
multi_pools = multiprocessing.Pool(10)
# run on a bunch of pools
df_list = multi_pools.map(model_by_cause, causes[0:30])
# concat mp jobs back together
test = pd.concat(df_list, sort=False, ignore_index=True)

# loop over causes and sexes
start = time.time()
counter = 0
counter_denom = causes.size
for cause in causes:
    for s in [1, 2]:
        # create the mask
        mask = (both["cause_code"] == cause) & (both["sex_id"] == s)
        if both[mask].log_rate.isnull().sum() == both[mask].shape[0]:
            print("there's no data")
            continue
        # our formula for predictions
        formula = "log_rate ~ C(age_start) + C(location_id)"
        # fit the model
        fit = smf.ols(formula, data=both[mask]).fit()
        # exponentiate the predicted values
        both.loc[mask, "preds"] = np.exp(fit.predict(both[mask]))
        if s == 1:
            counter += 1
            if counter % 125 == 0:
                print(round((counter / counter_denom) * 100, 1), "% Done")
                print("Run time: ", (time.time() - start) / 60, " minutes")

print("Done in ", (time.time() - start) / 60, " minutes")


# both.to_csv(r"H:/utla_lin_mod_output.csv")

###################################################

# both = back.copy()
# subtract off the existing cases that we have at utla level
# use a groupby transform to leave the data in same format but create sums of
# known values at the regional level
reg_groups = [
    "cause_code",
    "location_parent_id",
    "age_start",
    "age_end",
    "sex_id",
    "fiscal_year",
]

# fill missing utla level data with zeroes instead of NA so rows will be
# included in groupby
both["value"].fillna(value=0, inplace=True)

# sum the existing utla values up to the regional level
both["utla_val_to_reg"] = both.groupby(reg_groups)["value"].transform("sum")

# split the data
# subset the data to get only rows where utla value was suppressed
pred_df = both[both.utla_log_rate.isnull()].copy()
# drop the rows where utla value was suppressed
both = both[both.utla_log_rate.notnull()]

# subtract the known utla values from the regional values to get
# residual (unknown) values
pred_df["reg_resid_value"] = pred_df["reg_value"] - pred_df["utla_val_to_reg"]

# new method
# get into count space
pred_df["pred_counts"] = pred_df["preds"] * pred_df["utla_population"]

# sum utla predicted counts to region level
pred_df["utla_pred_to_reg"] = pred_df.groupby(reg_groups)["pred_counts"].transform("sum")

# make the weights
pred_df["weight"] = pred_df["reg_resid_value"] / pred_df["utla_pred_to_reg"]

# apply weights to predicted values
pred_df["weighted_counts"] = pred_df["pred_counts"] * pred_df["weight"]

# now test
reg_compare = pred_df.copy()
# get the sum of values at the regional level
reg_compare = reg_compare[
    [
        "cause_code",
        "location_parent_id",
        "age_start",
        "age_end",
        "sex_id",
        "fiscal_year",
        "reg_resid_value",
    ]
]
reg_compare.drop_duplicates(inplace=True)
reg_sum = reg_compare.reg_resid_value.sum()
# get the sum of desuppressed values
pred_df_sum = pred_df.weighted_counts.sum()
# pretty dang close to zero
assert round(reg_sum - pred_df_sum, 5) == 0

# assert residual vals are smaller than regional vals
assert (pred_df.reg_value >= pred_df.reg_resid_value).all()

# concat de-suppressed and un-suppressed data back together
both = pd.concat([both, pred_df], sort=False, ignore_index=True)

# merge data that needed to be de-suppressed and data that didn't into same col
# fill value with desuppressed val where value = 0 and desuppressed isn't null
condition = (both["value"] == 0) & (both["weighted_counts"].notnull())
both.loc[condition, "value"] = both.loc[condition, "weighted_counts"]

# write to a csv for use with a Shiny app
both["rates"] = both["value"] / both["utla_population"]

# write /ihme
both[
    [
        "location_id",
        "location_parent_id",
        "age_start",
        "age_end",
        "sex_id",
        "fiscal_year",
        "cause_code",
        "utla_log_rate",
        "value",
        "preds",
        "reg_value",
        "reg_resid_value",
        "weight",
    ]
].to_csv(
    "/ihme/hospital/clinical_data/inputs_from_j/UK_HOSPITAL_STATISTICS/intermediate_prep_and_wide_files/utla_desuppressed.csv",
    index=False,
)

both[
    [
        "location_id",
        "location_parent_id",
        "age_start",
        "age_end",
        "sex_id",
        "fiscal_year",
        "cause_code",
        "utla_log_rate",
        "value",
        "preds",
        "reg_value",
        "reg_resid_value",
        "weight",
        "rates",
        "utla_population",
    ]
].to_csv(
    root + r"/temp/hospital/2016/data/plotting_data/"
    r"utla_desuppressed_shiny_full_years_inp_only.csv",
    index=False,
)
