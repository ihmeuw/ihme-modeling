'''
Description: Contains tools that prepare data for the epi uploader.
Contents :
    EpiUploadDataframe : a class containing a dataset that is formatted for epi upload
    format_draws_data : accepts a dataframe and adds common values before using
        EpiUploadDataframe to prepare an Epi-ready dataframe
    load_seq_values : loads the previous seq values by uid to enable reassignment
Notes:
Contributors: INDIVIDUAL_NAME
'''
from numpy import nan
import pandas as pd
from cancer_estimation.py_utils import (
    common_utils as utils,
    modeled_locations,
)
from cancer_estimation.py_utils.gbd_cancer_tools import validate_proportions
import cancer_estimation._database.cdb_utils as cdb
from cancer_estimation.c_models.e_nonfatal import nonfatal_dataset as nd
import elmo
from db_queries import get_population
import subprocess
from datetime import datetime

class EpiUploadDataframe():
    ''' Holds the framework for an epi upload dataset
    '''
    epi_column_defaults = {'underlying_nid': nan,
                           'nid': utils.get_gbd_parameter('generic_cancer_nid'),
                           'field_citation_value': "",
                           'file_path': "",
                           'page_num': nan,
                           'table_num': nan,
                           'source_type': "Registry - cancer",
                           'location_name': "",
                           'smaller_site_unit': 1,
                           'site_memo': "generated by custom model",
                           'sex': nan,
                           'sex_issue': 0,
                           'year_start': nan,
                           'year_end': nan,
                           'year_issue': 0,
                           'age_start': nan,
                           'age_end': nan,
                           'age_issue': 0,
                           'age_demographer': 1,
                           'measure': "required_value",
                           'mean': "required_value",
                           'lower': nan,
                           'upper': nan,
                           'standard_error': nan,
                           'cases': nan,
                           'sample_size': nan,
                           'effective_sample_size': nan,
                           'design_effect': nan,
                           'unit_type': "Person",
                           'unit_value_as_published': 1,
                           'measure_issue': 0,
                           'measure_adjustment': 0,
                           'uncertainty_type': "",
                           'uncertainty_type_value': nan,
                           'representative_name': "Unknown",
                           'urbanicity_type': "Unknown",
                           'recall_type': "Not Set",
                           'recall_type_value': nan,
                           'sampling_type': nan,
                           'response_rate': nan,
                           'case_name': "",
                           'case_definition': "",
                           'case_diagnostics': "",
                           'group': nan,
                           'specificity': nan,
                           'group_review': nan,
                           'note_modeler': "",
                           'note_SR': "",
                           'input_type': "",
                           'outlier_type_id': 0,
                           'seq': "",
                           'extractor': "required_value"
                           }

    def __init__(self, epi_dataframe):
        ''' Populates and formats dataframe for upload to epi
        '''
        self.test_columns(epi_dataframe)
        self.data = self.format_epi_dataframe(epi_dataframe)
        self.test_columns(self.data)

    def test_columns(self, df):
        ''' Validates presence of required columns
        '''
        flexible_columns = {'year_start': 'year_id', 'year_end': 'year_id',
                            'age_start': 'age_group_id', 'age_end': 'age_group_id',
                            'sex': 'sex_id', 'location_name': 'location_id'}
        required_columns = \
            [v for v in self.epi_column_defaults.keys(
            ) if self.epi_column_defaults[v] == "required_value"]
        for col in flexible_columns.keys():
            if col not in df.columns and flexible_columns[col] not in df.columns:
                raise AssertionError("either {} or {} must be included in the"
                                     "epi dataframe")
        if not all(col in df.columns for col in required_columns):
            missing_cols = [
                col for col in required_columns if col not in df.columns]
            raise AssertionError(
                "Missing required columns: {}".format(missing_cols))
        for col in required_columns:
            if df[col].isnull().any():
                raise AssertionError(
                    "{} must have values in all rows".format(col))
        return (None)

    def format_epi_dataframe(self, df):
        ''' Tests and reformats columns as needed, then adds default values
                where possible
        '''
        # Check columns that likely need conversion and convert as necessary
        df = self.ensure_age_range(df)
        df = self.ensure_sex(df)
        df = self.ensure_year_range(df)
        df = self.ensure_location_info(df)
        df = self.ensure_nid(df)
        # Add any required columns without required values that are not already
        #   present
        defaults = self.epi_column_defaults
        cols_with_default = \
            [col for col in defaults if defaults[col] != "required_value"]
        for col in cols_with_default:
            if col not in df.columns:
                df[col] = defaults[col]
        df = df.loc[:, self.epi_column_defaults.keys()]
        return(df)

    def ensure_age_range(self, df):
        ''' If no age_start & age_end columns, uses age_group_id to add them
        '''
        age_range_cols = ['age_start', 'age_end']
        if all(a in df.columns for a in age_range_cols):
            if df[age_range_cols].notnull().all():
                return(df)
        # Update age, sex, and year
        df = df.loc[~df['age_group_id'].isin([22, 27]) &
                    (df['age_group_id'].eq(1) | (df['age_group_id'] >= 5)), :]
        df.loc[:, 'age_start'] = (df['age_group_id']-5).multiply(5)
        df.loc[df['age_group_id'] == 1, 'age_start'] = 0
        df.loc[df['age_group_id'] == 30, 'age_start'] = 80
        df.loc[df['age_group_id'] == 31, 'age_start'] = 85
        df.loc[df['age_group_id'] == 32, 'age_start'] = 90
        df.loc[df['age_group_id'] == 235, 'age_start'] = 95
        df['age_end'] = df['age_start'] + 4
        df.loc[df['age_group_id'] == 21, 'age_end'] = 99
        df.loc[df['age_group_id'] == 235, 'age_end'] = 130
        return (df)

    def ensure_sex(self, df):
        ''' If sex column is in sex_id format, updates values to Epi text format
        '''
        sex_map = {"1": "Male", "2": "Female", "3": "Both"}
        if 'sex_id' in df.columns and 'sex' not in df.columns:
            df['sex'] = df['sex_id'].astype(str)
        if df['sex'].isin(sex_map.values()).all():
            return (df)
        df.loc[:, 'sex'] = df['sex'].astype(str).map(sex_map)
        assert df['sex'].astype(str).isin(sex_map.values()).all(), \
            "Incorrect Sex Values Sent"
        return (df)

    def ensure_year_range(self, df):
        ''' If no year_start & year_end columns, uses year_id to add them
        '''
        year_range_cols = ['year_start', 'year_end']
        if all(y in df.columns for y in year_range_cols):
            if df[year_range_cols].notnull().all():
                return(df)
        df['year_start'] = df['year_id']
        df['year_end'] = df['year_id']
        return (df)

    def ensure_location_info(self, df):
        ''' If no location_name column, uses location_id to create it
        '''
        if 'location_name' in df.columns:
            if df['location_name'].notnull().all():
                return (df)
        df = modeled_locations.add_location_name(df)
        return(df)

    def ensure_nid(self, df):
        ''' Adds generic nid value if NID values are not yet present
        '''
        if 'nid' not in df.columns:
            df['nid'] = utils.get_gbd_parameter('generic_cancer_nid')
        else:
            df.loc[df['nid'].isnull(), 'nid'] = utils.get_gbd_parameter(
                'generic_cancer_nid')
        return (df)


def format_draws_data(epi_df):
    ''' Calculates the mean, lower, and upper estimates, then asserts NID before
        adding generic epi columns
    '''
    # Add uncertainty values and labels
    draw_cols = [c for c in epi_df.columns if c.startswith('draw')]
    epi_df['mean'] = epi_df[draw_cols].mean(axis=1)
    epi_df['upper'] = epi_df[draw_cols].quantile(q=0.95, axis=1)
    epi_df['lower'] = epi_df[draw_cols].quantile(q=0.05, axis=1)
    epi_df['uncertainty_type'] = "Confidence interval"
    epi_df['uncertainty_type_value'] = 95
    return(epi_df)

def hotfix_spikes(df):
    ''' Detects draws with rates > 1. assigns the average of all non-problematic draws 
        to the draw that has an entry > 1 
    '''
    print('bad column found. applying hotfix...')
    draw_cols = ['draw_{}'.format(i) for i in list(range(0,1000))]
    bad_cols = []
    
    # get list of draws that have value(s) > 1 
    for draw in draw_cols: 
        if df[draw].gt(1).any(): 
            bad_cols += [draw]
    # remove the troublesome draws from the dataframe 
    for re in bad_cols: 
        draw_cols.remove(re)
    # re-insert draw by calculating average of draws that were kept 
    for bc in bad_cols: 
        df[bc] = df[draw_cols].mean(axis=1)
    return(df)

def convert_to_rate(df, cols_to_convert, location_id, acause):
    ''' Returns a formatted dataframe of the IHME population estimate for the
        given location_id
    '''
    count_cols = [c + '_count' for c in cols_to_convert]
    pop_uids = ['location_id', 'year_id', 'sex_id', 'age_group_id']
    
    # get decomp_step 
    d_step = utils.get_gbd_parameter('current_decomp_step')
    pop_df = pd.read_csv('{}/population.csv'.format(utils.get_path(process='nonfatal_model', key='database_cache')))
    pop_df = pop_df.loc[pop_df['location_id'].eq(location_id), ]
    pop_df = pop_df[pop_uids + ['population']]
    rate_df = df.merge(pop_df)
    assert not rate_df['population'].isnull().any(), "Missing population values"
    rate_df[count_cols] = rate_df[cols_to_convert]
    rate_df.loc[:, cols_to_convert] =\
        rate_df[cols_to_convert].divide(rate_df['population'], axis=0)
    rate_df['est_err'] = 0
    for col in count_cols:
        rate_df.loc[rate_df['population'].lt(1) &
                rate_df[col].gt(rate_df['population']), 'est_err'] = 1
    rate_df.loc[rate_df['est_err'].eq(1), 'upper'] = 1
    rate_df.loc[rate_df['est_err'].eq(1), 'lower'] = 0
    rate_df.loc[rate_df['est_err'].eq(1), 'mean'] = 0
    if ((rate_df[cols_to_convert].gt(1).any().any()) & (acause == "neo_liver_hbl")):
        rate_df = hotfix_spikes(rate_df)

    loc = '_'.join([str(x) for x in rate_df.location_id.unique().tolist()])
    validate_proportions(rate_df[cols_to_convert])
    rate_df.drop(['population', 'est_err'], axis=1, inplace=True)
    return (rate_df)


def load_previous_seq_values(bundle_id):
    ''' Returns any previous seq values for the bundle_id
    '''
    print("    downloading previous seq values...")
    try:
        df = elmo.get_epi_data(bundle_id)
        if df.at[0, 'processing'] == 1:
            df = pd.DataFrame()
    except:
        df = pd.DataFrame()
    return(df)


def delete_previous_data(bundle_id, acause):
    ''' Deletes previous model data by downloading it and re-uploading as an empty
        file
    '''
    bundle_dir = "{}/{}/{}/01_input_data/01_nonlit".format(
        utils.get_path('epi_bundle_staging', process="cancer_model"),acause, bundle_id)
    upload_file = "{}/{}_delete_previous_{}.xlsx".format(
        bundle_dir, bundle_id, utils.display_timestamp())
    # Download dataframe and set all values as empty
    seq_df = load_previous_seq_values(bundle_id)
    if len(seq_df) < 1:
        return (pd.DataFrame())
    else:
        print("    removing previous inputs...")
        for col in seq_df.columns:
            if col != 'seq':
                seq_df.loc[:, col] = nan
        seq_df.to_excel(upload_file, sheet_name="extraction", index=False)
        df = elmo.upload_epi_data(bundle_id, upload_file)
        return(df)


def save_epi_input_file(df, filepath):
    ''' Saves file per criteria specific to the epi uploader
    '''
    assert filepath[-5:] == ".xlsx", "Data must be saved as xlsx file."
    print("saving epi input data to {}...".format(filepath))
    # set smallest value that can be saved with pd.DataFrame().to_excel method
    floor = 1e-12
    # Because the uploader will not accept mean = upper = lower = 0 (floor),
    #   set upper bound for such cases to the minimum value above zero (floor)
    cant_upload = (df['lower'].le(floor) & df['mean'].le(floor) &
                   df['upper'].le(floor))
    df.loc[cant_upload, 'upper'] = df.loc[df['upper'].gt(floor), 'upper'].min()
    # A precision error exists in pandas to_excel. Cannot write numbers smaller
    #       than the floor
    for col in ['mean', 'lower', 'upper']:
        too_small = df[col].between(0, floor, inclusive=False)  # keep "true" zero
        df.loc[too_small, col] = floor
    df.to_excel(filepath, sheet_name="extraction",index=False, encoding='utf-8')
    return


def upload_epi_dataset(bundle_id, filepath):
    ''' Accepts a dataframe and bundle_id, validates the data, then uploads them
            to Epi
    '''
    d_step = utils.get_gbd_parameter('current_decomp_step')
    log_folder = "{}/{}_epi_upload".format(
        utils.get_path("cancer_logs"), bundle_id)
    utils.ensure_dir(log_folder)
    test = elmo.validate_input_sheet(bundle_id, filepath, log_folder)
    print(bundle_id, filepath)
    df = elmo.upload_bundle_data(bundle_id, d_step, filepath)
    return(df)


def update_upload_record(me_id, model_run_id, tracking_id, crosswalk_id, cancer_model_type, success):
    ''' Adds entry for the successful upload to the cancer database
    '''
    db_link = cdb.db_api()
    this_me = db_link.get_entry("cnf_model_entity", "modelable_entity_id", me_id)
    me_tag = this_me.at[0, 'me_tag']
    acause = this_me.at[0, 'acause']
    description = "{}, {}".format(acause, me_tag)
    if cancer_model_type == "custom_dismod":
        id_type = 'request_id'
    elif "custom_epi" in cancer_model_type:
        id_type = "model_version_id"
    # add upload record
    update_dict = {'cnf_model_version_id': model_run_id, 'tracking_id': tracking_id,
                   'tracking_id_type': id_type, 'modelable_entity_id': me_id,
                    'cnf_upload_description': description, 'successful_upload':success,
                    'crosswalk_version_id' : crosswalk_id
                   }
    missing_info = any(value_ is None for value_ in update_dict.values())
    assert not missing_info, "Missing info required to update database"
    upload_record = pd.DataFrame(update_dict, index=[0])
    # 2024-08-08 save the update_dict as a csv while we're not sure uploading to our database works
    timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
    cnf_entry_filepath = f'{utils.get_path("cnf_upload_entries_dir", process="nonfatal_model")}/cnf_upload_entry_{me_id}_{model_run_id}_{tracking_id}_{crosswalk_id}_{cancer_model_type}_{timestamp}.csv'
    upload_record.to_csv(cnf_entry_filepath)
    # upload the update_dict to our database, as we've always done
    db_link.append_to_table('cnf_model_upload', upload_record)
    print(f"Upload record updated for run {model_run_id} me {me_id}")
    return(True)
