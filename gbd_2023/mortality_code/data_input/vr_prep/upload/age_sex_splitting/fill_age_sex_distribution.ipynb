{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fill out age sex splitting distribution\n",
    "need to make sure all new locations are in there\n",
    "\n",
    "- every location has the same years\n",
    "- should be square\n",
    "- all the new subnational locations have a parent location in the old distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import db_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_locations(df, new_location_ids):\n",
    "    missing_locations = list(set(new_location_ids) - set(df.location_id.unique()))\n",
    "    if missing_locations:\n",
    "        print(\"There are missing locations\")\n",
    "        print(missing_locations)\n",
    "    return missing_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_location_metadata = db_queries.get_location_metadata(gbd_round_id=6, location_set_id=82)\n",
    "old_location_metadata = db_queries.get_location_metadata(gbd_round_id=5, location_set_id=82)\n",
    "\n",
    "new_location_ids = list(set(new_location_metadata.location_id) - set(old_location_metadata.location_id))\n",
    "\n",
    "input_distribution_file = (\"\")\n",
    "\n",
    "old_distribution = pd.read_csv(input_distribution_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_locations(old_distribution, new_location_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_locations_only_metadata = new_location_metadata[new_location_metadata.location_id.isin(new_location_ids)]\n",
    "\n",
    "new_parent_location_ids = new_locations_only_metadata[new_locations_only_metadata.level == 4].parent_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old_distribution[old_distribution.location_id.isin(new_parent_location_ids)].location_id.unique())\n",
    "print(new_parent_location_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ~~I want to make a dictionary that goes parent_id : subnational id~~\n",
    "2. Then for a parent_id, get the subset of the old distribution that has that parent id\n",
    "3. then take that subset, and replace the parent_id location_id with one of the child location_ids\n",
    "4. repeat for each child location\n",
    "5. append all child location dataframes together.\n",
    "6. append that to the distribution file\n",
    "\n",
    "parent-to-children dict allows me to run this for each parent id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parent_location_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parent_location_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_new_subnationals(distribution_df, parent_id):\n",
    "    \n",
    "    # subset distribution df to just the dist for the parent_id\n",
    "    parent_distribution_df = distribution_df[distribution_df.location_id == parent_id].copy()\n",
    "    assert parent_distribution_df.shape[0] > 0, \"NO DATA FOR THIS LOCATION: {}\".foramt(parent_id)\n",
    "    \n",
    "    # get location_metadata for the children locations of the parent and test them\n",
    "    children_location_metadata = new_location_metadata[\n",
    "        new_location_metadata.path_to_top_parent.str.contains(\",{},\".format(parent_id))\n",
    "    ].copy()\n",
    "    assert (children_location_metadata.level == 4).all()\n",
    "    # get list of children location ids\n",
    "    children_location_ids = children_location_metadata.location_id.unique().tolist()\n",
    "    \n",
    "    # could do a location_id change and merge but this way I can check each df as it comes\n",
    "    # For each child location:\n",
    "    #   1. make a copy of the parent_distribution df\n",
    "    #   2. set the location id in the copy to the child location\n",
    "    #   3. assert that dataframes are identical, besides the location_id\n",
    "    #   4. append to the list of dataframes\n",
    "    children_distribution_dfs_list = []\n",
    "    for child_loc in children_location_ids:\n",
    "        child_distribution_df = parent_distribution_df.copy()\n",
    "        child_distribution_df.location_id = child_loc\n",
    "        pd.testing.assert_frame_equal(left=child_distribution_df.drop(\"location_id\", axis=1),\n",
    "                                      right=parent_distribution_df.drop(\"location_id\", axis=1),\n",
    "                                      check_like=False)\n",
    "        children_distribution_dfs_list.append(child_distribution_df)\n",
    "    assert(len(children_distribution_dfs_list) == len(children_distribution_dfs_list))\n",
    "    \n",
    "    child_distribution_df = pd.concat(children_distribution_dfs_list, ignore_index=True)\n",
    "    assert child_distribution_df.shape[0] > 0\n",
    "    assert child_distribution_df.shape[0] == parent_distribution_df.shape[0] * len(children_distribution_dfs_list)\n",
    "    assert set(child_distribution_df.location_id) == set(children_location_ids)\n",
    "    assert not child_distribution_df.duplicated().any()\n",
    "    return child_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phl_sub = make_weights_for_new_subnationals(distribution_df=old_distribution, parent_id=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a child distribution df for each of the new parent_ids\n",
    "children_distribution_dfs_list = []\n",
    "for parent_id in new_parent_location_ids:\n",
    "    children_distribution_dfs_list.append(\n",
    "        make_weights_for_new_subnationals(distribution_df=old_distribution, parent_id=parent_id)\n",
    "    )\n",
    "assert len(children_distribution_dfs_list) == len(new_parent_location_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_subnational_locations_dist_df = pd.concat(\n",
    "    children_distribution_dfs_list, ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_subnational_locations_dist_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_distribution = pd.concat(\n",
    "    [old_distribution, all_new_subnational_locations_dist_df], ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_locations(df=new_distribution, new_location_ids=new_location_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_location_information(location):\n",
    "    \n",
    "    location_name = new_location_metadata[\n",
    "        new_location_metadata.location_id == location\n",
    "    ].location_name.unique()[0]\n",
    "    \n",
    "    parent_id = new_location_metadata[\n",
    "        new_location_metadata.location_id == location\n",
    "    ].parent_id.unique()[0]\n",
    "    \n",
    "    parent_location_name = new_location_metadata[\n",
    "        new_location_metadata.location_id == parent_id\n",
    "    ].location_name.unique()[0]\n",
    "    \n",
    "    print(\"Location: {}\".format(location))\n",
    "    print(\"location_name: {}\".format(location_name))\n",
    "    print(\"parent_id: {}\".format(parent_id))\n",
    "    print(\"parent_id location_name: {}\".format(parent_location_name))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [320, 416, 393, 396, 367, 369, 374, 380, 413]:\n",
    "    print_location_information(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_distribution[old_distribution.location_id == 104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oceania_locs = old_location_metadata[old_location_metadata.parent_id == 21].location_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = db_queries.get_population(gbd_round_id=6, location_id=oceania_locs, age_group_id=22, sex_id=3, year_id=2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pop.merge(new_location_metadata[['location_id', 'location_name']], how='left', on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.sort_values(\"population\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a dictionary mapping new_location to reference location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_locations_only_metadata[\n",
    "        new_locations_only_metadata.parent_id == 21\n",
    "    ].location_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing locations\n",
    "\n",
    "[320, 416, 393, 396, 367, 369, 374, 380, 413]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([320, 369, 374, 380, 413, 416]).symmetric_difference(set([320, 416, 393, 396, 367, 369, 374, 380, 413]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [367, 393, 396]:\n",
    "    print_location_information(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_metadata.query(\"location_name == 'Antigua and Barbuda'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_new_to_old_loc_map = {\n",
    "    # Oceania locations use Paupa New Guinea\n",
    "    320: 26,\n",
    "    369: 26,\n",
    "    374: 26,\n",
    "    380: 26,\n",
    "    413: 26,\n",
    "    416: 26,\n",
    "    # for San Marino use Italy\n",
    "    396: 86,\n",
    "    # for Monaco use France\n",
    "    367: 80,\n",
    "    # for Saint Kitts and Nevis use Antigua and Barbuda\n",
    "    393: 105\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mapping\n",
    "for key in dist_new_to_old_loc_map:\n",
    "    new_location_name = new_location_metadata[\n",
    "        new_location_metadata.location_id == key\n",
    "    ].location_name.unique()[0]\n",
    "    \n",
    "    old_location_name = new_location_metadata[\n",
    "        new_location_metadata.location_id == dist_new_to_old_loc_map[key]\n",
    "    ].location_name.unique()[0]\n",
    "    \n",
    "    print(\"{} : {}\".format(location_name, old_location_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_new_locations(distribution_df, new_location_id, dist_new_to_old_loc_map):\n",
    "    \n",
    "    reference_location_id = dist_new_to_old_loc_map[new_location_id]\n",
    "    \n",
    "    new_location_name = new_location_metadata[\n",
    "        new_location_metadata.location_id == new_location_id\n",
    "    ].location_name.unique()[0]\n",
    "    reference_location_name = new_location_metadata[\n",
    "        new_location_metadata.location_id == reference_location_id\n",
    "    ].location_name.unique()[0]\n",
    "    \n",
    "    print(\"Giving {} the distribution of {}\".format(new_location_name, reference_location_name))\n",
    "    print(\"Giving {} the distribution of {}\".format(new_location_id, reference_location_id))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # subset distribution df to just the dist for the reference location\n",
    "    reference_distribution_df = distribution_df[distribution_df.location_id == reference_location_id].copy()\n",
    "    assert reference_distribution_df.shape[0] > 0, \"NO DATA FOR THIS LOCATION: {}\".foramt(reference_location_id)\n",
    "    \n",
    "    # replace location_id in reference_distribution_df with the location id of the new location\n",
    "    reference_distribution_df.location_id = new_location_id\n",
    "    print(reference_distribution_df.shape)\n",
    "    return reference_distribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist_new_to_old_loc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_locations_distribution_df_list = []\n",
    "for key in dist_new_to_old_loc_map:\n",
    "    new_locations_distribution_df_list.append(\n",
    "        make_weights_for_new_locations(old_distribution, key, dist_new_to_old_loc_map)\n",
    "    )\n",
    "assert len(new_locations_distribution_df_list) == len(dist_new_to_old_loc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_locations_dist_df = pd.concat(new_locations_distribution_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_distribution = pd.concat([old_distribution,\n",
    "                              new_locations_dist_df,\n",
    "                              all_new_subnational_locations_dist_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_locations(df=new_distribution, new_location_ids=new_location_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_distribution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not new_distribution.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not new_distribution.drop(\"weight\", axis=1).duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_location_metadata = db_queries.get_location_metadata(gbd_round_id=6, location_set_id=82)\n",
    "old_location_metadata = db_queries.get_location_metadata(gbd_round_id=5, location_set_id=82)\n",
    "new_location_ids = list(set(new_location_metadata.location_id) - set(old_location_metadata.location_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(new_location_ids) < set(new_distribution.location_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_distribution.to_csv(\n",
    "    ,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
